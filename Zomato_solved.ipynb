{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "np.random.seed(1) # set a seed so that the results are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "\n",
    "    s = 1/(1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for setting the structure of Network\n",
    "def layer_sizes(X, Y):\n",
    "    \n",
    "    n_x = X.shape[0]  #Input layer nodes  \n",
    "    n_h = 4 #total hidden layer\n",
    "    n_y = 6 #output layer nodes\n",
    "    \n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the parameters\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "        \n",
    "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x)) ##\n",
    "    assert (b1.shape == (n_h, 1))  ## To confirm the shapes of Parameters\n",
    "    assert (W2.shape == (n_y, n_h)) ## As these are the main reason of most errors\n",
    "    assert (b2.shape == (n_y, 1))  ##\n",
    "    \n",
    "    parameters = {\"W1\": W1, ##\n",
    "                  \"b1\": b1, ## Putting all parameters together so that\n",
    "                  \"W2\": W2, ## we do not end up with confiusing due to many parameters\n",
    "                  \"b2\": b2} ##\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation\n",
    "def forward_propagation(X, parameters):\n",
    "    \n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    b1 = parameters['b1']\n",
    "    b2 = parameters['b2']\n",
    "   \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    #assert(A2.shape == (6, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,  \n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    cost = -np.sum((Y * np.log(A2) + (1 - Y) * np.log(1 - A2)))/m\n",
    "    \n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward Propagation\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dz2 = A2 - Y\n",
    "    dW2 = np.dot(dz2, A1.T)/m\n",
    "    db2 = np.sum(dz2, axis = 1, keepdims = True)/m\n",
    "    \n",
    "    dz1 = np.dot(W2.T, dz2) * (1 - np.power(A1, 2))\n",
    "    dW1 = np.dot(dz1, X.T)/m\n",
    "    db1 = np.sum(dz1, axis = 1, keepdims = True)/m\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    b1 = parameters['b1']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads['dW1']\n",
    "    dW2 = grads['dW2']\n",
    "    db1 = grads['db1']\n",
    "    db2 = grads['db2']    \n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    W1 = W1 - (learning_rate * dW1)\n",
    "    W2 = W2 - (learning_rate * dW2)\n",
    "    b1 = b1 - (learning_rate * db1)\n",
    "    b2 = b2 - (learning_rate * db2)\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=True):\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies.\n",
    "    train_y, cache = forward_propagation(X, parameters)\n",
    "    predicted_y = np.zeros_like(train_y)\n",
    "    max_term = train_y.argmax(0)\n",
    "\n",
    "    for i in range(X.shape[1]):\n",
    "        predicted_y[max_term[i], i] = 1\n",
    "    \n",
    "    ratings = np.array(['Not rated', 'Poor', 'Average', 'Good', 'Very Good', 'Excellent'])\n",
    "    predicted_ratings = np.zeros((1, X.shape[1]))\n",
    "    predicted_ratings = pd.Series(ratings[predicted_y.argmax(0)])\n",
    "    \n",
    "    return predicted_y, predicted_ratings\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above were for trainig the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below cells are for Preprocessing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    \n",
    "    \n",
    "    y = data['Rating text']\n",
    "    X = data.copy()\n",
    "\n",
    "    text_col = ['Restaurant Name', 'City', 'Address', 'Cuisines', 'Rating color', 'Rating text', 'Currency',\n",
    "                'Locality Verbose', 'Locality']\n",
    "    for i in text_col:\n",
    "        del X[i]\n",
    "\n",
    "    ratings = ['Not rated', 'Poor', 'Average', 'Good', 'Very Good', 'Excellent']\n",
    "\n",
    "    print(\"Type of ratings : \", ratings)\n",
    "\n",
    "    \n",
    "    for i in range(len(ratings)):\n",
    "        y = y.replace(to_replace = ratings[i], value= i)\n",
    "\n",
    "    sm_column = ['Has Table booking', 'Has Online delivery', 'Switch to order menu', 'Is delivering now']\n",
    "    \n",
    "    \n",
    "    for i in sm_column:\n",
    "        X[i] = X[i].replace(to_replace = 'Yes', value = 1)\n",
    "        X[i] = X[i].replace(to_replace = 'No', value = 0)\n",
    "\n",
    "    \n",
    "    #Feature scaling of data using\n",
    "    scaler = StandardScaler()\n",
    "    x_scaler = scaler.fit_transform(X)\n",
    "    \n",
    "    \n",
    "    new_data = pd.DataFrame(x_scaler, columns= X.columns)\n",
    "    new_data['Rating'] = y\n",
    "    \n",
    "    return new_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(data):\n",
    "\n",
    "    new_data = preprocessing(data)\n",
    "    \n",
    "# Splitting The training data and test data\n",
    "    X = np.array(new_data.iloc[:, :new_data.shape[1] - 1]).T\n",
    "    n, m = X.shape\n",
    "    \n",
    "    X_train = X[:, :6999]\n",
    "    X_test = X[:, 7000: m-1]\n",
    "    \n",
    "    # encoding the data for multiclassification\n",
    "    \n",
    "    Y = np.zeros((6,6))\n",
    "    diag = np.eye(6)\n",
    "    y = np.array(new_data.iloc[:, new_data.shape[1] - 1], dtype = int)\n",
    "    Y = diag[y].T\n",
    "    \n",
    "    Y_train = Y[:, :6999]\n",
    "    Y_test = Y[:, 7000: m-1]\n",
    "    \n",
    "# Text data to match at the end and getting Accuracy\n",
    "    y_text_train = data['Rating text'][:6999]\n",
    "    y_text_train = y_text_train.reshape(y_text_train.shape[0], 1)\n",
    "\n",
    "    y_text_test = data['Rating text'][7000: 9550]\n",
    "    y_text_test = y_text_test.reshape(y_text_test.shape[0], 1)\n",
    "    \n",
    "    print(m, n)\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test, y_text_train, y_text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Type of ratings : ', ['Not rated', 'Poor', 'Average', 'Good', 'Very Good', 'Excellent'])\n",
      "(9551, 12)\n",
      "('X_train shape : ', (12, 6999), <type 'numpy.ndarray'>)\n",
      "('X_test shape : ', (12, 2550), <type 'numpy.ndarray'>)\n",
      "('Y_train shape : ', (6, 6999), <type 'numpy.ndarray'>)\n",
      "('Y_test shape : ', (6, 2550), <type 'numpy.ndarray'>)\n",
      "('y_text_train shape : ', (6999, 1), <type 'numpy.ndarray'>)\n",
      "('y_text_test shape : ', (2550, 1), <type 'numpy.ndarray'>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:24: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:27: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('zomato.csv')\n",
    "X_train, Y_train, X_test, Y_test, y_text_train, y_text_test = dataset(data)\n",
    "print(\"X_train shape : \", X_train.shape, type(X_train))\n",
    "print(\"X_test shape : \", X_test.shape, type(X_test))\n",
    "print(\"Y_train shape : \", Y_train.shape, type(X_train))\n",
    "print(\"Y_test shape : \", Y_test.shape, type(X_test))\n",
    "print(\"y_text_train shape : \", y_text_train.shape, type(X_train))\n",
    "print(\"y_text_test shape : \", y_text_test.shape, type(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start training the parameters and predict the data, training of the parameters will take 3 to 4 minutes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 4.158918\n",
      "Cost after iteration 1000: 0.230243\n",
      "Cost after iteration 2000: 0.184716\n",
      "Cost after iteration 3000: 0.167386\n",
      "Cost after iteration 4000: 0.162962\n",
      "Cost after iteration 5000: 0.160279\n",
      "Cost after iteration 6000: 0.158443\n",
      "Cost after iteration 7000: 0.157080\n",
      "Cost after iteration 8000: 0.156012\n",
      "Cost after iteration 9000: 0.155146\n"
     ]
    }
   ],
   "source": [
    "# Train the parameters\n",
    "parameters = nn_model(X_train, Y_train, n_h = 4, num_iterations = 10000, print_cost=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y_train, predicted_ratings_train = predict(parameters, X_train)\n",
    "predicted_ratings_train = predicted_ratings_train.values.reshape(predicted_ratings_train.shape[0], 1)\n",
    "\n",
    "predicted_y_test, predicted_ratings_test = predict(parameters, X_test)\n",
    "predicted_ratings_test = predicted_ratings_test.values.reshape(predicted_ratings_test.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy on training set : ', 96.82811830261467)\n",
      "('Accuracy on training set : ', 96.50980392156863)\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy on training set : ', np.mean(predicted_ratings_train == y_text_train) * 100)\n",
    "print('Accuracy on training set : ', np.mean(predicted_ratings_test == y_text_test) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### A file named 'new.csv' in folder has the preprocessed data of 'zomato.csv'. For Giving you own data to predict. Use data from 'new.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('new.csv', header = None)\n",
    "X_new_data = np.array(new_data.iloc[:, :new_data.shape[1] - 1].T)\n",
    "#real_y = np.array(new_data.iloc[:, new_data.shape[1] -1], dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prediction : Good']\n",
      "real value : Good\n"
     ]
    }
   ],
   "source": [
    "real_y = data['Rating text']\n",
    "# Example of precdition\n",
    "i = 7000\n",
    "a, prediction = predict(parameters, X_new_data[:, i].reshape(12,1))\n",
    "prediction = np.array(prediction)\n",
    "print('prediction : ' + prediction)\n",
    "print('real value : ' + real_y[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try yourself by changing the value of i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
